{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark-Jupyter Notebook Mac Setup  \n",
    "-Source: https://medium.com/@GalarnykMichael/install-spark-on-mac-pyspark-453f395f240b  \n",
    "\n",
    "1. Download Anaconda (choose verion for your desired python distribution): https://www.anaconda.com/distribution/  \n",
    "> - Follow installation steps  \n",
    "> - Do NOT install pyspark package, this will cause source conflict issues  \n",
    "\n",
    "2. Download Java8, the latest version of Java supported by Spark  \n",
    "> - With Homebrew: type <b>brew cask install adoptopenjdk/openjdk/adoptopenjdk8</b> in terminal    \n",
    "> - If another version of Java is installed, be sure to set you JAVA_HOME environment variable in the terminal or your bash_profile:  \n",
    "<b>export JAVA_HOME=/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home</b>  \n",
    "> - Alternatively you can set the Java version using: <b>export JAVA_HOME=$(/usr/libexec/java_home -v 1.8)</b>  \n",
    "\n",
    "3. Download Apache Spark binaries: http://spark.apache.org/downloads.html  \n",
    "> - Unzip and move to your home folder (you can navigate there by typing <b>cd ~'</b> in the terminal)  \n",
    "> - While in your home directory type <b>nano .bash_profile</b> and add the following lines:  \n",
    "<b>export SPARK_PATH=~/[YOUR_SPARK_VERSION_FOLDER]  \n",
    "export PYSPARK_DRIVER_PYTHON=\"jupyter\"  \n",
    "export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\"  \n",
    "alias snotebook='$SPARK_PATH/bin/pyspark --master local[2]'</b>  \n",
    "> - To finalize changes type <b>source .bash_profile</b>  \n",
    "> - Note: you can replace \"snotebook\" with whatever command you want to use to activate a Spark notebook  \n",
    "\n",
    "4. Now open a terminal and simply type <b>snotebook</b> to get started in Spark with Jupyter notebook!    \n",
    "  \n",
    "5. You may see an error about loading native Hadoop library, this will not affect functionality but if you want to fix this:  \n",
    "> - Download your chosen Hadoop version BINARY from Apache: http://hadoop.apache.org/  \n",
    "> - Unzip and move to your home directory  \n",
    "> - Add the following to your bash_profile: <b>export HADOOP_HOME=~/hadoop-2.8.0</b> with your corresponding version name  \n",
    "> - Restart the terminal and the warning should be gone!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit, col, unix_timestamp, dayofyear, hour, minute, dayofmonth\n",
    "from pyspark.sql.functions import isnull,date_trunc, window, countDistinct, desc, to_utc_timestamp, from_unixtime\n",
    "from pyspark.sql.types import DateType, TimestampType, LongType, IntegerType, DoubleType, FloatType\n",
    "from pyspark.sql import DataFrame, Row\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, IndexToString, StringIndexer,\\\n",
    "VectorIndexer, VectorAssembler, StandardScaler, CountVectorizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "import keras\n",
    "from keras.layers import LSTM, RepeatVector, TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import optimizers\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c02yt01vlvdt.localdomain:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1185b6c88>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c02yt01vlvdt.localdomain:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=PySparkShell>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)  \n",
    "The dataset used in this notebook is CICIDS2017 from University of New Brunswick, which contains attacks on real-world network data. The dataset can be downloaded here: https://www.unb.ca/cic/datasets/ids-2017.html. The breakdown of the data is as follows:  \n",
    "  \n",
    "Monday has no attacks, only normal network flow  \n",
    "Tuesday-Friday all have different attacks associated with certain timestamps    \n",
    "  \n",
    "<b>Note:</b> This is not a full EDA, and in real-world scenarios you should spend much more time understanding the dataset you are working with. This cheatsheet helped immensely in my Spark ramp-up as well: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------+------------------+-----------------------+---------------------------+----------------------------+----------------------+----------------------+-----------------------+----------------------+---------------------+----------------------+-----------------------+----------------------+------------+---------------+--------------+-------------+-------------+-------------+-------------+-------------+------------+------------+------------+-------------+-------------+------------+------------+------------+-------------+--------------+--------------+--------------+--------------------+------------------+-------------+--------------+------------------+------------------+-------------------+------------------+-----------------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+--------------+--------------------+---------------------+---------------------+--------------------+------------------+---------------------+------------------+-------------------+---------------------+-----------------+-------------------+------------------+--------------------+------------------+----------------------+------------------------+-----------------+---------------------+-----------+-----------+-----------+-----------+---------+---------+---------+---------+------+\n",
      "| Destination Port| Flow Duration| Total Fwd Packets| Total Backward Packets|Total Length of Fwd Packets| Total Length of Bwd Packets| Fwd Packet Length Max| Fwd Packet Length Min| Fwd Packet Length Mean| Fwd Packet Length Std|Bwd Packet Length Max| Bwd Packet Length Min| Bwd Packet Length Mean| Bwd Packet Length Std|Flow Bytes/s| Flow Packets/s| Flow IAT Mean| Flow IAT Std| Flow IAT Max| Flow IAT Min|Fwd IAT Total| Fwd IAT Mean| Fwd IAT Std| Fwd IAT Max| Fwd IAT Min|Bwd IAT Total| Bwd IAT Mean| Bwd IAT Std| Bwd IAT Max| Bwd IAT Min|Fwd PSH Flags| Bwd PSH Flags| Fwd URG Flags| Bwd URG Flags| Fwd Header Length34| Bwd Header Length|Fwd Packets/s| Bwd Packets/s| Min Packet Length| Max Packet Length| Packet Length Mean| Packet Length Std| Packet Length Variance|FIN Flag Count| SYN Flag Count| RST Flag Count| PSH Flag Count| ACK Flag Count| URG Flag Count| CWE Flag Count| ECE Flag Count| Down/Up Ratio| Average Packet Size| Avg Fwd Segment Size| Avg Bwd Segment Size| Fwd Header Length55|Fwd Avg Bytes/Bulk| Fwd Avg Packets/Bulk| Fwd Avg Bulk Rate| Bwd Avg Bytes/Bulk| Bwd Avg Packets/Bulk|Bwd Avg Bulk Rate|Subflow Fwd Packets| Subflow Fwd Bytes| Subflow Bwd Packets| Subflow Bwd Bytes|Init_Win_bytes_forward| Init_Win_bytes_backward| act_data_pkt_fwd| min_seg_size_forward|Active Mean| Active Std| Active Max| Active Min|Idle Mean| Idle Std| Idle Max| Idle Min| Label|\n",
      "+-----------------+--------------+------------------+-----------------------+---------------------------+----------------------------+----------------------+----------------------+-----------------------+----------------------+---------------------+----------------------+-----------------------+----------------------+------------+---------------+--------------+-------------+-------------+-------------+-------------+-------------+------------+------------+------------+-------------+-------------+------------+------------+------------+-------------+--------------+--------------+--------------+--------------------+------------------+-------------+--------------+------------------+------------------+-------------------+------------------+-----------------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+--------------+--------------------+---------------------+---------------------+--------------------+------------------+---------------------+------------------+-------------------+---------------------+-----------------+-------------------+------------------+--------------------+------------------+----------------------+------------------------+-----------------+---------------------+-----------+-----------+-----------+-----------+---------+---------+---------+---------+------+\n",
      "|               88|           640|                 7|                      4|                        440|                         358|                   220|                     0|            62.85714286|            107.349008|                  179|                     0|                   89.5|           103.3456982|     1246875|        17187.5|            64|   135.557286|          445|            1|          640|  106.6666667| 194.3251571|         497|           1|          538|  179.3333333| 303.6862416|         530|           4|            0|             0|             0|             0|                 164|               104|      10937.5|          6250|                 0|               220|               66.5|       99.00183653|            9801.363636|             0|              0|              0|              1|              0|              0|              0|              0|             0|         72.54545455|          62.85714286|                 89.5|                 164|                 0|                    0|                 0|                  0|                    0|                0|                  7|               440|                   4|               358|                  8192|                    2053|                2|                   20|          0|          0|          0|          0|        0|        0|        0|        0|BENIGN|\n",
      "|               88|           900|                 9|                      4|                        600|                        2944|                   300|                     0|            66.66666667|           132.2875656|                 1472|                     0|                    736|           849.8595962| 3937777.778|    14444.44444|            75|  192.7952282|          684|            1|          900|        112.5| 252.4112291|         734|           1|          766|  255.3333333| 435.3232515|         758|           3|            0|             0|             0|             0|                 204|               104|        10000|   4444.444444|                 0|              1472|        253.1428571|       527.4342623|            278186.9011|             0|              0|              0|              1|              0|              0|              0|              0|             0|         272.6153846|          66.66666667|                  736|                 204|                 0|                    0|                 0|                  0|                    0|                0|                  9|               600|                   4|              2944|                  8192|                    2053|                2|                   20|          0|          0|          0|          0|        0|        0|        0|        0|BENIGN|\n",
      "|               88|          1205|                 7|                      4|                       2776|                        2830|                  1388|                     0|            396.5714286|           677.2746506|                 1415|                     0|                  707.5|           816.9506309| 4652282.158|    9128.630705|         120.5|  236.4333357|          777|            1|         1205|  200.8333333|  397.058392|        1008|           1|          927|          309| 491.6472313|         876|           1|            0|             0|             0|             0|                 164|               104|  5809.128631|   3319.502075|                 0|              1415|        467.1666667|       690.0989169|            476236.5152|             0|              0|              0|              1|              0|              0|              0|              0|             0|         509.6363636|          396.5714286|                707.5|                 164|                 0|                    0|                 0|                  0|                    0|                0|                  7|              2776|                   4|              2830|                  8192|                    2053|                2|                   20|          0|          0|          0|          0|        0|        0|        0|        0|BENIGN|\n",
      "|               88|           511|                 7|                      4|                        452|                         370|                   226|                     0|            64.57142857|           110.2767082|                  185|                     0|                   92.5|           106.8097998| 1608610.568|    21526.41879|          51.1|  90.76765209|          299|            1|          511|  85.16666667| 131.9005939|         349|           1|          462|          154| 224.7198256|         412|           1|            0|             0|             0|             0|                 164|               104|  13698.63014|    7827.78865|                 0|               226|               68.5|       101.9335791|            10390.45455|             0|              0|              0|              1|              0|              0|              0|              0|             0|         74.72727273|          64.57142857|                 92.5|                 164|                 0|                    0|                 0|                  0|                    0|                0|                  7|               452|                   4|               370|                  8192|                    2053|                2|                   20|          0|          0|          0|          0|        0|        0|        0|        0|BENIGN|\n",
      "|               88|           773|                 9|                      4|                        612|                        2944|                   306|                     0|                     68|           134.9333169|                 1472|                     0|                    736|           849.8595962| 4600258.732|    16817.59379|   64.41666667|  148.6982658|          531|            1|          773|       96.625| 196.6657335|         580|           1|          675|          225| 348.9011321|         627|           1|            0|             0|             0|             0|                 204|               104|  11642.94955|   5174.644243|                 0|              1472|                254|       527.5207615|            278278.1538|             0|              0|              0|              1|              0|              0|              0|              0|             0|         273.5384615|                   68|                  736|                 204|                 0|                    0|                 0|                  0|                    0|                0|                  9|               612|                   4|              2944|                  8192|                    2053|                2|                   20|          0|          0|          0|          0|        0|        0|        0|        0|BENIGN|\n",
      "+-----------------+--------------+------------------+-----------------------+---------------------------+----------------------------+----------------------+----------------------+-----------------------+----------------------+---------------------+----------------------+-----------------------+----------------------+------------+---------------+--------------+-------------+-------------+-------------+-------------+-------------+------------+------------+------------+-------------+-------------+------------+------------+------------+-------------+--------------+--------------+--------------+--------------------+------------------+-------------+--------------+------------------+------------------+-------------------+------------------+-----------------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+--------------+--------------------+---------------------+---------------------+--------------------+------------------+---------------------+------------------+-------------------+---------------------+-----------------+-------------------+------------------+--------------------+------------------+----------------------+------------------------+-----------------+---------------------+-----------+-----------+-----------+-----------+---------+---------+---------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://www.unb.ca/cic/datasets/ids-2017.html\n",
    "\n",
    "#Change the url to match folder location for the data\n",
    "df1 = spark.read.csv('UNB_IDSdata/MachineLearningCVE/Tuesday-WorkingHours.pcap_ISCX.csv',sep=',',header=True)\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' Flow Duration', 'string')\n",
      "\n",
      "\n",
      " (' Flow Duration', 'float')\n"
     ]
    }
   ],
   "source": [
    "#Currently all data is stored as strings, a first step would be to cast relevant columns to integer/float\n",
    "print(df1.dtypes[1])\n",
    "\n",
    "\n",
    "#Cast all columns to float, EXCEPT leave label as string for now (deal with it later)\n",
    "#Destination Port is also left as a string, which will be explained later\n",
    "df1 = df1.select(*(col(c).cast(\"float\").alias(c) if (c!=' Label' and c!=' Destination Port') else col(c) for c in df1.columns))\n",
    "print(\"\\n\\n\",df1.dtypes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are a bunch of columns, making it difficult to visualize the data especially in Spark. A subset of columns can be taken to show some of the data exploration functionality in Spark. The 'decribe' method prints statistics associated with each column (however, this dataset actually includes some of these statistics as extra columns by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+---------------------------+\n",
      "|summary| Subflow Bwd Bytes|Total Length of Fwd Packets|\n",
      "+-------+------------------+---------------------------+\n",
      "|  count|            445909|                     445909|\n",
      "|   mean| 21861.62704273742|          530.2105429583166|\n",
      "| stddev| 2625473.325022229|          5676.904047554064|\n",
      "|    min|               0.0|                        0.0|\n",
      "|    max|       6.2703904E8|                  2428415.0|\n",
      "+-------+------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Selects two columns, determines statistical measures, and prints\n",
    "df1.select(' Subflow Bwd Bytes','Total Length of Fwd Packets').describe().show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains mostly continuous columns and only have a few Categorical columns:  \n",
    "\n",
    "    Destination Port  \n",
    "    Fwd PSH Flags, Bwd PSH Flags, Fwd URG Flags, Bwd URG Flags  \n",
    "    label (our truth label for the data)  \n",
    "    \n",
    "label needs to be dealt with separately, but we can examine our other categorical columns to decide how we should handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique ports:  31858\n",
      "+-----------------+------------------+\n",
      "| Destination Port|dstport_occurences|\n",
      "+-----------------+------------------+\n",
      "|               53|            190939|\n",
      "|              443|             97276|\n",
      "|               80|             45676|\n",
      "|               21|              8996|\n",
      "|               22|              8033|\n",
      "|              123|              5145|\n",
      "|              137|              1911|\n",
      "|              389|              1333|\n",
      "|               88|              1232|\n",
      "|              465|               710|\n",
      "|             3268|               607|\n",
      "|              139|               564|\n",
      "|              445|               372|\n",
      "|              138|               318|\n",
      "|                0|               316|\n",
      "|              135|               178|\n",
      "|            49666|               172|\n",
      "|             5353|               148|\n",
      "|             5355|               102|\n",
      "|            49671|                69|\n",
      "+-----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select the categorical columns, we can then find the cardinality of each and popular values\n",
    "categories = df1.select(' Destination Port', 'Fwd PSH Flags', ' Bwd PSH Flags', ' Fwd URG Flags', ' Bwd URG Flags', \" Label\")\n",
    "\n",
    "#This call will give us counts of Destination ports ordered highest to lowest\n",
    "port_counts = categories.groupby(' Destination Port').agg(func.count('Fwd PSH Flags').alias('dstport_occurences'))\\\n",
    ".sort(col('dstport_occurences').desc())\n",
    "\n",
    "print(\"Number of unique ports: \",port_counts.count())\n",
    "port_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_cols = [' Destination Port', 'Fwd PSH Flags', ' Bwd PSH Flags', ' Fwd URG Flags', ' Bwd URG Flags', \" Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique  Destination Port:  31858\n",
      "+-----------------+----------------------------+\n",
      "| Destination Port| Destination Port_occurences|\n",
      "+-----------------+----------------------------+\n",
      "|               53|                      190939|\n",
      "|              443|                       97276|\n",
      "|               80|                       45676|\n",
      "|               21|                        8996|\n",
      "|               22|                        8033|\n",
      "|              123|                        5145|\n",
      "|              137|                        1911|\n",
      "|              389|                        1333|\n",
      "|               88|                        1232|\n",
      "|              465|                         710|\n",
      "|             3268|                         607|\n",
      "|              139|                         564|\n",
      "|              445|                         372|\n",
      "|              138|                         318|\n",
      "|                0|                         316|\n",
      "|              135|                         178|\n",
      "|            49666|                         172|\n",
      "|             5353|                         148|\n",
      "|             5355|                         102|\n",
      "|            49671|                          69|\n",
      "+-----------------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of unique Fwd PSH Flags:  2\n",
      "+-------------+------------------------+\n",
      "|Fwd PSH Flags|Fwd PSH Flags_occurences|\n",
      "+-------------+------------------------+\n",
      "|          0.0|                  418832|\n",
      "|          1.0|                   27077|\n",
      "+-------------+------------------------+\n",
      "\n",
      "Number of unique  Bwd PSH Flags:  1\n",
      "+--------------+-------------------------+\n",
      "| Bwd PSH Flags| Bwd PSH Flags_occurences|\n",
      "+--------------+-------------------------+\n",
      "|           0.0|                   445909|\n",
      "+--------------+-------------------------+\n",
      "\n",
      "Number of unique  Fwd URG Flags:  1\n",
      "+--------------+-------------------------+\n",
      "| Fwd URG Flags| Fwd URG Flags_occurences|\n",
      "+--------------+-------------------------+\n",
      "|           0.0|                   445909|\n",
      "+--------------+-------------------------+\n",
      "\n",
      "Number of unique  Bwd URG Flags:  1\n",
      "+--------------+-------------------------+\n",
      "| Bwd URG Flags| Bwd URG Flags_occurences|\n",
      "+--------------+-------------------------+\n",
      "|           0.0|                   445909|\n",
      "+--------------+-------------------------+\n",
      "\n",
      "Number of unique  Label:  3\n",
      "+-----------+-----------------+\n",
      "|      Label| Label_occurences|\n",
      "+-----------+-----------------+\n",
      "|     BENIGN|           432074|\n",
      "|FTP-Patator|             7938|\n",
      "|SSH-Patator|             5897|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To speed our experimentation, the following code will loop through each categorical column and print its cardinality\n",
    "#and top values:\n",
    "for c in categ_cols:\n",
    "    counts = categories.groupby(c).agg(func.count(' Destination Port').alias(c+'_occurences'))\\\n",
    "    .sort(col(c+'_occurences').desc())\n",
    "\n",
    "    print(\"Number of unique \"+c+\": \",counts.count())\n",
    "    counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above distributions, we should discard BWD PSH and both \"URG\" features, as they provide no distinguishing information about the records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering & Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the above analysis, the Flags variables seem to have binary values, with Ports being the only categorical column with very high cardinality. We will likely have to create a new feature for ports, since one-hot encoding would create too many new columns. \n",
    "  \n",
    "In this case, we can use the number of times each port shows up in normal traffic to create \"usage bins\" for ports, such that bins with similar number of occurences are seen as similar to the algorithm. There should also be a bin for ports that have not been seen before to avoid erroring upon seeing new port behavior. To do this, we will use the <b>Bucketizer</b> class from PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucketizer output with 8 buckets\n",
      "+-----------------+------------------+-------------+\n",
      "| Destination Port|dstport_occurences|bucketedPorts|\n",
      "+-----------------+------------------+-------------+\n",
      "|               53|            190939|          7.0|\n",
      "|              443|             97276|          6.0|\n",
      "|               80|             45676|          6.0|\n",
      "|               21|              8996|          5.0|\n",
      "|               22|              8033|          5.0|\n",
      "|              123|              5145|          5.0|\n",
      "|              137|              1911|          4.0|\n",
      "|              389|              1333|          4.0|\n",
      "|               88|              1232|          4.0|\n",
      "|              465|               710|          3.0|\n",
      "|             3268|               607|          3.0|\n",
      "|              139|               564|          3.0|\n",
      "|              445|               372|          3.0|\n",
      "|              138|               318|          3.0|\n",
      "|                0|               316|          3.0|\n",
      "|              135|               178|          3.0|\n",
      "|            49666|               172|          3.0|\n",
      "|             5353|               148|          3.0|\n",
      "|             5355|               102|          3.0|\n",
      "|            49671|                69|          2.0|\n",
      "+-----------------+------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "#This defines the split values for our buckets, these can be adjusted, and I based them \n",
    "#roughly on the counts from above\n",
    "splits = [0, 1, 50, 100, 1000, 5000, 10000, 100000, float('Inf')]\n",
    "\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"dstport_occurences\",\\\n",
    "                        outputCol=\"bucketedPorts\")\n",
    "\n",
    "# Transform original data into its bucket index.\n",
    "bucketedData = bucketizer.setHandleInvalid(\"keep\").transform(port_counts)\n",
    "\n",
    "##Essentially, what we have created is a mapping from port number -> its usage bucket below,\n",
    "##we can use this on incoming data to assign ports to buckets\n",
    "print(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits())-1))\n",
    "bucketedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This step is to avoid naming issues in the following join\n",
    "bucketedData = bucketedData.withColumn(\"dstport\", col(\" Destination Port\"))\n",
    "#bucketedData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+-----------------------+---------------------------+----------------------------+----------------------+----------------------+-----------------------+----------------------+---------------------+----------------------+-----------------------+----------------------+------------+---------------+--------------+-------------+-------------+-------------+-------------+-------------+------------+------------+------------+-------------+-------------+------------+------------+------------+-------------+--------------+--------------+--------------+--------------------+------------------+-------------+--------------+------------------+------------------+-------------------+------------------+-----------------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+--------------+--------------------+---------------------+---------------------+--------------------+------------------+---------------------+------------------+-------------------+---------------------+-----------------+-------------------+------------------+--------------------+------------------+----------------------+------------------------+-----------------+---------------------+-----------+-----------+-----------+-----------+---------+---------+---------+---------+------+-------------+\n",
      "| Flow Duration| Total Fwd Packets| Total Backward Packets|Total Length of Fwd Packets| Total Length of Bwd Packets| Fwd Packet Length Max| Fwd Packet Length Min| Fwd Packet Length Mean| Fwd Packet Length Std|Bwd Packet Length Max| Bwd Packet Length Min| Bwd Packet Length Mean| Bwd Packet Length Std|Flow Bytes/s| Flow Packets/s| Flow IAT Mean| Flow IAT Std| Flow IAT Max| Flow IAT Min|Fwd IAT Total| Fwd IAT Mean| Fwd IAT Std| Fwd IAT Max| Fwd IAT Min|Bwd IAT Total| Bwd IAT Mean| Bwd IAT Std| Bwd IAT Max| Bwd IAT Min|Fwd PSH Flags| Bwd PSH Flags| Fwd URG Flags| Bwd URG Flags| Fwd Header Length34| Bwd Header Length|Fwd Packets/s| Bwd Packets/s| Min Packet Length| Max Packet Length| Packet Length Mean| Packet Length Std| Packet Length Variance|FIN Flag Count| SYN Flag Count| RST Flag Count| PSH Flag Count| ACK Flag Count| URG Flag Count| CWE Flag Count| ECE Flag Count| Down/Up Ratio| Average Packet Size| Avg Fwd Segment Size| Avg Bwd Segment Size| Fwd Header Length55|Fwd Avg Bytes/Bulk| Fwd Avg Packets/Bulk| Fwd Avg Bulk Rate| Bwd Avg Bytes/Bulk| Bwd Avg Packets/Bulk|Bwd Avg Bulk Rate|Subflow Fwd Packets| Subflow Fwd Bytes| Subflow Bwd Packets| Subflow Bwd Bytes|Init_Win_bytes_forward| Init_Win_bytes_backward| act_data_pkt_fwd| min_seg_size_forward|Active Mean| Active Std| Active Max| Active Min|Idle Mean| Idle Std| Idle Max| Idle Min| Label|bucketedPorts|\n",
      "+--------------+------------------+-----------------------+---------------------------+----------------------------+----------------------+----------------------+-----------------------+----------------------+---------------------+----------------------+-----------------------+----------------------+------------+---------------+--------------+-------------+-------------+-------------+-------------+-------------+------------+------------+------------+-------------+-------------+------------+------------+------------+-------------+--------------+--------------+--------------+--------------------+------------------+-------------+--------------+------------------+------------------+-------------------+------------------+-----------------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+--------------+--------------------+---------------------+---------------------+--------------------+------------------+---------------------+------------------+-------------------+---------------------+-----------------+-------------------+------------------+--------------------+------------------+----------------------+------------------------+-----------------+---------------------+-----------+-----------+-----------+-----------+---------+---------+---------+---------+------+-------------+\n",
      "|         640.0|               7.0|                    4.0|                      440.0|                       358.0|                 220.0|                   0.0|              62.857143|             107.34901|                179.0|                   0.0|                   89.5|            103.345695|   1246875.0|        17187.5|          64.0|    135.55728|        445.0|          1.0|        640.0|   106.666664|   194.32515|       497.0|         1.0|        538.0|    179.33333|   303.68625|       530.0|         4.0|          0.0|           0.0|           0.0|           0.0|               164.0|             104.0|      10937.5|        6250.0|               0.0|             220.0|               66.5|          99.00184|               9801.363|           0.0|            0.0|            0.0|            1.0|            0.0|            0.0|            0.0|            0.0|           0.0|           72.545456|            62.857143|                 89.5|               164.0|               0.0|                  0.0|               0.0|                0.0|                  0.0|              0.0|                7.0|             440.0|                 4.0|             358.0|                8192.0|                  2053.0|              2.0|                 20.0|        0.0|        0.0|        0.0|        0.0|      0.0|      0.0|      0.0|      0.0|BENIGN|          4.0|\n",
      "|         900.0|               9.0|                    4.0|                      600.0|                      2944.0|                 300.0|                   0.0|              66.666664|             132.28757|               1472.0|                   0.0|                  736.0|              849.8596|   3937777.8|      14444.444|          75.0|    192.79523|        684.0|          1.0|        900.0|        112.5|   252.41122|       734.0|         1.0|        766.0|    255.33333|   435.32324|       758.0|         3.0|          0.0|           0.0|           0.0|           0.0|               204.0|             104.0|      10000.0|     4444.4443|               0.0|            1472.0|          253.14285|         527.43427|               278186.9|           0.0|            0.0|            0.0|            1.0|            0.0|            0.0|            0.0|            0.0|           0.0|            272.6154|            66.666664|                736.0|               204.0|               0.0|                  0.0|               0.0|                0.0|                  0.0|              0.0|                9.0|             600.0|                 4.0|            2944.0|                8192.0|                  2053.0|              2.0|                 20.0|        0.0|        0.0|        0.0|        0.0|      0.0|      0.0|      0.0|      0.0|BENIGN|          4.0|\n",
      "|        1205.0|               7.0|                    4.0|                     2776.0|                      2830.0|                1388.0|                   0.0|              396.57144|             677.27466|               1415.0|                   0.0|                  707.5|              816.9506|   4652282.0|       9128.631|         120.5|    236.43333|        777.0|          1.0|       1205.0|    200.83333|   397.05838|      1008.0|         1.0|        927.0|        309.0|   491.64722|       876.0|         1.0|          0.0|           0.0|           0.0|           0.0|               164.0|             104.0|    5809.1284|      3319.502|               0.0|            1415.0|          467.16666|         690.09894|               476236.5|           0.0|            0.0|            0.0|            1.0|            0.0|            0.0|            0.0|            0.0|           0.0|           509.63635|            396.57144|                707.5|               164.0|               0.0|                  0.0|               0.0|                0.0|                  0.0|              0.0|                7.0|            2776.0|                 4.0|            2830.0|                8192.0|                  2053.0|              2.0|                 20.0|        0.0|        0.0|        0.0|        0.0|      0.0|      0.0|      0.0|      0.0|BENIGN|          4.0|\n",
      "|         511.0|               7.0|                    4.0|                      452.0|                       370.0|                 226.0|                   0.0|               64.57143|             110.27671|                185.0|                   0.0|                   92.5|              106.8098|   1608610.6|      21526.418|          51.1|    90.767654|        299.0|          1.0|        511.0|    85.166664|   131.90059|       349.0|         1.0|        462.0|        154.0|   224.71982|       412.0|         1.0|          0.0|           0.0|           0.0|           0.0|               164.0|             104.0|     13698.63|     7827.7886|               0.0|             226.0|               68.5|         101.93358|              10390.454|           0.0|            0.0|            0.0|            1.0|            0.0|            0.0|            0.0|            0.0|           0.0|            74.72727|             64.57143|                 92.5|               164.0|               0.0|                  0.0|               0.0|                0.0|                  0.0|              0.0|                7.0|             452.0|                 4.0|             370.0|                8192.0|                  2053.0|              2.0|                 20.0|        0.0|        0.0|        0.0|        0.0|      0.0|      0.0|      0.0|      0.0|BENIGN|          4.0|\n",
      "|         773.0|               9.0|                    4.0|                      612.0|                      2944.0|                 306.0|                   0.0|                   68.0|             134.93332|               1472.0|                   0.0|                  736.0|              849.8596|   4600258.5|      16817.594|     64.416664|    148.69827|        531.0|          1.0|        773.0|       96.625|   196.66574|       580.0|         1.0|        675.0|        225.0|   348.90112|       627.0|         1.0|          0.0|           0.0|           0.0|           0.0|               204.0|             104.0|    11642.949|      5174.644|               0.0|            1472.0|              254.0|         527.52075|              278278.16|           0.0|            0.0|            0.0|            1.0|            0.0|            0.0|            0.0|            0.0|           0.0|           273.53845|                 68.0|                736.0|               204.0|               0.0|                  0.0|               0.0|                0.0|                  0.0|              0.0|                9.0|             612.0|                 4.0|            2944.0|                8192.0|                  2053.0|              2.0|                 20.0|        0.0|        0.0|        0.0|        0.0|      0.0|      0.0|      0.0|      0.0|BENIGN|          4.0|\n",
      "|         986.0|               9.0|                    6.0|                     3100.0|                      3030.0|                1550.0|                   0.0|              344.44446|              683.4858|               1515.0|                   0.0|                  505.0|             782.34265|   6217038.5|      15212.981|      70.42857|    182.10847|        696.0|          1.0|        986.0|       123.25|   268.41425|       785.0|         1.0|        889.0|        177.8|   300.83084|       697.0|         1.0|          0.0|           0.0|           0.0|           0.0|               204.0|             144.0|     9127.789|      6085.193|               0.0|            1550.0|            383.125|          685.4144|               469792.9|           0.0|            0.0|            0.0|            1.0|            0.0|            0.0|            0.0|            0.0|           0.0|           408.66666|            344.44446|                505.0|               204.0|               0.0|                  0.0|               0.0|                0.0|                  0.0|              0.0|                9.0|            3100.0|                 6.0|            3030.0|                8192.0|                  2053.0|              2.0|                 20.0|        0.0|        0.0|        0.0|        0.0|      0.0|      0.0|      0.0|      0.0|BENIGN|          4.0|\n",
      "|         935.0|               9.0|                    6.0|                     3074.0|                      3010.0|                1537.0|                   0.0|              341.55554|              677.7533|               1505.0|                   0.0|              501.66666|             777.17865|   6506952.0|       16042.78|      66.78571|    187.36192|        714.0|          1.0|        935.0|      116.875|   281.53125|       811.0|         1.0|        859.0|        171.8|    308.5226|       714.0|         2.0|          0.0|           0.0|           0.0|           0.0|               204.0|             144.0|     9625.668|     6417.1123|               0.0|            1537.0|             380.25|          680.2621|              462756.47|           0.0|            0.0|            0.0|            1.0|            0.0|            0.0|            0.0|            0.0|           0.0|               405.6|            341.55554|            501.66666|               204.0|               0.0|                  0.0|               0.0|                0.0|                  0.0|              0.0|                9.0|            3074.0|                 6.0|            3010.0|                8192.0|                  2053.0|              2.0|                 20.0|        0.0|        0.0|        0.0|        0.0|      0.0|      0.0|      0.0|      0.0|BENIGN|          4.0|\n",
      "|      572849.0|              15.0|                   12.0|                     4076.0|                      1020.0|                1646.0|                   0.0|              271.73334|             562.93585|                210.0|                   0.0|                   85.0|              91.22201|    8895.887|       47.13284|     22032.654|    101642.21|     517763.0|          2.0|     572849.0|    40917.785|   137973.11|    517763.0|         2.0|     572795.0|    52072.273|   172091.12|    570946.0|         3.0|          0.0|           0.0|           0.0|           0.0|               324.0|             264.0|    26.184912|      20.94793|               0.0|            1646.0|              182.0|         421.41315|              177589.03|           0.0|            0.0|            0.0|            1.0|            0.0|            0.0|            0.0|            0.0|           0.0|           188.74074|            271.73334|                 85.0|               324.0|               0.0|                  0.0|               0.0|                0.0|                  0.0|              0.0|               15.0|            4076.0|                12.0|            1020.0|                8192.0|                     0.0|              8.0|                 20.0|        0.0|        0.0|        0.0|        0.0|      0.0|      0.0|      0.0|      0.0|BENIGN|          4.0|\n",
      "|           1.0|               2.0|                    1.0|                        0.0|                         0.0|                   0.0|                   0.0|                    0.0|                   0.0|                  0.0|                   0.0|                    0.0|                   0.0|         0.0|      3000000.0|           0.5|   0.70710677|          1.0|          0.0|          1.0|          1.0|         0.0|         1.0|         1.0|          0.0|          0.0|         0.0|         0.0|         0.0|          0.0|           0.0|           0.0|           0.0|                40.0|              20.0|    2000000.0|     1000000.0|               0.0|               0.0|                0.0|               0.0|                    0.0|           1.0|            0.0|            0.0|            0.0|            0.0|            0.0|            0.0|            0.0|           0.0|                 0.0|                  0.0|                  0.0|                40.0|               0.0|                  0.0|               0.0|                0.0|                  0.0|              0.0|                2.0|               0.0|                 1.0|               0.0|                   0.0|                   254.0|              0.0|                 20.0|        0.0|        0.0|        0.0|        0.0|      0.0|      0.0|      0.0|      0.0|BENIGN|          1.0|\n",
      "|        1075.0|               9.0|                    9.0|                     3100.0|                      3030.0|                1550.0|                   0.0|              344.44446|              683.4858|               1515.0|                   0.0|              336.66666|              668.0522|   5702325.5|      16744.186|     63.235294|     167.8878|        709.0|          1.0|       1075.0|      134.375|   255.32664|       762.0|         1.0|       1009.0|      126.125|   243.25732|       709.0|         1.0|          0.0|           0.0|           0.0|           0.0|               204.0|             204.0|     8372.093|      8372.093|               0.0|            1550.0|           322.6316|          641.9466|              412095.47|           0.0|            0.0|            0.0|            1.0|            0.0|            0.0|            0.0|            0.0|           1.0|           340.55554|            344.44446|            336.66666|               204.0|               0.0|                  0.0|               0.0|                0.0|                  0.0|              0.0|                9.0|            3100.0|                 9.0|            3030.0|                8192.0|                     0.0|              2.0|                 20.0|        0.0|        0.0|        0.0|        0.0|      0.0|      0.0|      0.0|      0.0|BENIGN|          4.0|\n",
      "+--------------+------------------+-----------------------+---------------------------+----------------------------+----------------------+----------------------+-----------------------+----------------------+---------------------+----------------------+-----------------------+----------------------+------------+---------------+--------------+-------------+-------------+-------------+-------------+-------------+------------+------------+------------+-------------+-------------+------------+------------+------------+-------------+--------------+--------------+--------------+--------------------+------------------+-------------+--------------+------------------+------------------+-------------------+------------------+-----------------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+--------------+--------------------+---------------------+---------------------+--------------------+------------------+---------------------+------------------+-------------------+---------------------+-----------------+-------------------+------------------+--------------------+------------------+----------------------+------------------------+-----------------+---------------------+-----------+-----------+-----------+-----------+---------+---------+---------+---------+------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####join tables: df1 and bucketedData to get bucketedPorts in df1\n",
    "df = df1.join(bucketedData.select(\"dstport\", \"bucketedPorts\"), df1[\" Destination Port\"]==bucketedData['dstport'])\n",
    "\n",
    "##Sanity Check\n",
    "#df4.groupBy('bucketedPorts').count().orderBy('count').show()\n",
    "\n",
    "\n",
    "#Drop the categorical values now that we have our buckets\n",
    "df = df.drop(\" Destination Port\", \"dstport\")\n",
    "df = df.withColumn(\"bucketedPorts\", col(\"bucketedPorts\").cast(\"float\"))\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, we need to deal with the plethora of continuous variables we have. Essentially, we can first check if any of them are too highly correlated as a sanity check. Next, we would want to vectorize using PySpark's <b> VectorAssembler</b> and scale them with <b>StandardScaler</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_cols = ['bucketedPorts', 'Fwd PSH Flags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[' Fwd Avg Bulk Rate', ' Bwd Avg Packets/Bulk', ' Active Std', ' Max Packet Length', ' ECE Flag Count', ' Flow IAT Max', ' Flow Packets/s', ' min_seg_size_forward', ' Subflow Bwd Bytes', ' Fwd IAT Min', 'Fwd Packets/s', 'FIN Flag Count', 'Total Length of Fwd Packets', ' Flow Duration', ' Fwd IAT Mean', ' URG Flag Count', 'Bwd IAT Total', ' Packet Length Std', ' Min Packet Length', ' Idle Max', ' Bwd Avg Bytes/Bulk', ' Avg Bwd Segment Size', ' act_data_pkt_fwd', ' Fwd Packet Length Min', 'Flow Bytes/s', ' Total Fwd Packets', ' Packet Length Variance', ' ACK Flag Count', ' Fwd IAT Max', ' Total Length of Bwd Packets', ' Bwd Header Length', ' Fwd Packet Length Max', ' Bwd IAT Std', 'Bwd Packet Length Max', ' Fwd Packet Length Std', ' Bwd IAT Max', 'Bwd Avg Bulk Rate', ' Flow IAT Std', ' Subflow Bwd Packets', ' RST Flag Count', ' Flow IAT Mean', ' Fwd Avg Packets/Bulk', ' Bwd Packet Length Mean', ' Active Max', ' Fwd IAT Std', 'Subflow Fwd Packets', ' SYN Flag Count', ' Idle Std', ' Subflow Fwd Bytes', ' Active Min', 'Idle Mean', ' CWE Flag Count', ' Avg Fwd Segment Size', ' Total Backward Packets', 'Fwd Avg Bytes/Bulk', ' PSH Flag Count', ' Flow IAT Min', ' Bwd Packet Length Std', ' Init_Win_bytes_backward', ' Bwd Packets/s', 'Init_Win_bytes_forward', ' Down/Up Ratio', ' Bwd Packet Length Min', ' Fwd Packet Length Mean', ' Fwd Header Length55', 'Fwd IAT Total', ' Average Packet Size', ' Idle Min', ' Bwd IAT Mean', 'Active Mean', ' Packet Length Mean', ' Fwd Header Length34', ' Bwd IAT Min']\n"
     ]
    }
   ],
   "source": [
    "##remember that \"cols\" holds our categorical columns from above, we add labels to it as well and then filter such that\n",
    "##only the continuous columns are vectorized at this time\n",
    "cont_cols = list((set(df.columns) - set(categ_cols)) - set([\" Label\"]))\n",
    "\n",
    "\n",
    "#Running this locally I actually ran out of RAM, on a cluster this likely will not happen to you, but one workaround\n",
    "#Is manually chunking operations\n",
    "cont_cols1 = cont_cols[20:30]\n",
    "#cont_cols2 = cont_cols[int(round((len(cont_cols)/3))):int(round(2*(len(cont_cols)/3)))]\n",
    "#cont_cols3 = cont_cols[int(round(2*(len(cont_cols)/3))):]\n",
    "\n",
    "print(len(cont_cols1))\n",
    "print(cont_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This gathers all our continuous variables to one vector column named \"features\", now we can use StandardScaler or \n",
    "#any other scaling/normalization function on this column\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=cont_cols1,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid='skip')\n",
    "vectorizedDF = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I create and plot the correlation matrix for the first 10 variables. Unfortunately I am working locally so getting the full correlation matrix proved impossible, but it is a good approach to filter extraneous columns like this. PCA is another approach available in PySpark natively, check this link to learn more: https://spark.apache.org/docs/2.2.0/ml-features.html#pca. However in some applications, PCA ruins the context of the data, even if achieving better accuracy it is hard to explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to examine the correlations of these variables, to avoid highly correlated variables which can\n",
    "#increase bias in the model\n",
    "from pyspark.ml.stat import Correlation\n",
    "df_vector = vectorizedDF.select(\"features\")\n",
    "corr_matrix = Correlation.corr(df_vector, \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a39d98d68>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAD8CAYAAABTjp5OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAZBklEQVR4nO3df5QdZX3H8fcnS4gGokRCAZMgQaP15wFMo609FOWHsbWgbSnBtgQPuNiKP+qpCtJKwdKiVBFPqU2UUKw2Uai2W01FhKJtFdxFwo8EkBAQtgFBg9AWhOzut3/MRG8ue/femzszu8/cz4szJ3Pn1/e5BL777DPPzFcRgZmZzXyzprsBZmbWGSdsM7NEOGGbmSXCCdvMLBFO2GZmiXDCNjNLhBO2mVkLktZKekjSbS32S9InJW2RdIukwxv2rZJ0V76sKqI9TthmZq39PbBiiv1vAJbmyyDwKQBJzwHOAV4FLAfOkTS/18Y4YZuZtRAR3wK2T3HI8cBnI3M9sI+kA4HXA1dHxPaIeAS4mqkTf0f26PUC7ez40dbKHqWcveCQqkKZWTnU6wW6yTl77vf808l6xjutiYg1XYRbCNzf8Hk039Zqe09KT9hmZjNVnpy7SdDNJvsBE1Ns74mHRMysXibGO196Nwosbvi8CNg2xfaeOGGbWb2Mj3W+9G4IODmfLfJq4NGIeAC4CjhW0vz8ZuOx+baeeEjEzGolYqKwa0laBxwJLJA0SjbzY3YWJ/4O2AD8OrAFeBx4a75vu6QPA8P5pc6LiKluXnbWnrJfr+qbjmbWhZ5vOj41emvnNx0XvbzneFVyD9vM6qXAHvZM44RtZvVSzM3EGckJ28zqpZ972JJ+kexpnoVk8wi3AUMRcXvJbTMz61oUM/tjRppyWp+kDwDryW4EfJfsjqeAdZLOLL95ZmZdmpjofElMux72qcBLI2JH40ZJHwc2ARdMdpKkQfLHPf/2Y3/BaSefVEBTzcw60MdDIhPAc4EfNG0/MN83qcbHPauc1mdm1s83Hd8DXCPpLn7+IpODgBcAZ5TZMDOz3dKvPeyI+JqkF5K9z3Uh2fj1KDAcEfX9MWZm6arxTce2s0Qie87z+graYmbWuwRvJnbK87DNrFbq/Mu/E7aZ1Uu/jmGbmSXHQyJmZolwD9vMLBHjO9ofkygnbDOrFw+JpGHHj7ZWEueal36wkjgAn5zzWGWxADbct6GSOO89eGUlcc6/+PBK4gDMO+HiymJdeMBrK4t136zq5jVfdO/63i/iIZHdV1UVmKqStZnNcO5hm5klwgnbzCwN4ZuOZmaJ8Bi2mVkiPCRiZpYI97DNzBLhHraZWSLcwzYzS8RYfQsYTFk1fSqS3lpkQ8zMChETnS+J2e2EDZzbaoekQUkjkkbWrFnTQwgzsy5NTHS+JGbKIRFJt7TaBezf6rzGqumAq6abWXUS7Dl3qt0Y9v7A64FHmrYL+HYpLTIz60WCPedOtUvYXwH2joiNzTskXVdKi8zMetGvPeyIOHWKfW8pvjlmZj3yLBEzs0REdL60IWmFpDslbZF05iT7L5K0MV++L+knDfvGG/YNFfHVPA/bzOqloDFsSQPAJcAxwCgwLGkoIjbvPCYi/rjh+HcChzVc4omIOLSQxuTcwzazeiluWt9yYEtEbI2Ip4D1wPFTHH8SsK6gbzEpJ2wzq5cuHpxpfGYkXwYbrrQQuL/h82i+7WkkPQ9YAlzbsPkZ+TWvl/SmIr6ah0TMrF7Gxzs+tOmZkWaa7JQWx64EroyIxuAHRcQ2SYcA10q6NSLu7rhxk6hVwq6qOO5Rm/6ykjgAFx/2jspiVelZMVBJnOHThyuJA/CKfZdUFmtuhTPX9pqV2C/ixc3DHgUWN3xeBGxrcexKYJf/WSNiW/7n1nwa9GFATwk7sb+J1qqsZG5mM1hxY9jDwFJJSyTtSZaUnzbbQ9KLgPnAdxq2zZc0J19fALwG2Nx8brdq1cM2MyvqwZmIGJN0BnAVMACsjYhNks4DRiJiZ/I+CVgfscs8wRcDqyVNkHWML2icXbK7nLDNrFZiorjXF0XEBmBD07YPNX3+80nO+zbw8sIaknPCNrN66eN3iZiZpaWLWSKpccI2s3pxD9vMLBFO2GZmiejgpU6pcsI2s3qpcQ+77YMzkn5R0lGS9m7avqK8ZpmZ7aaJ6HxJzJQJW9K7gH8B3gncJqnxTVXVPZ9tZtap8fHOl8S062G/DXhlRLwJOBL4M0nvzvdN9mKUbIerppvZNImJiY6X1LQbwx6IiP8FiIh7JR0JXJm/SrBlwnbVdDObNgkOdXSqXQ/7QUk/q5iQJ+83Agso4bFLM7OedfE+7NS062GfDOxS0TIixoCTJa0urVVmZrurxj3sdlXTR6fY91/FN8fMrEdj6d1M7JTnYZtZvSQ41NEpJ2wzq5d+HRIxM0tNitP1OuWEbWb14h62mVkinLBnvk/OeayyWFVWMh+66ZLKYlXpHv20kjgfGNheSRyATT/+QWWxrt1/QWWx5lJNhfvCJPjIeadqk7DNzKDYmo4zjRO2mdWLE7aZWSI8S8TMLBHuYZuZJcIJ28wsDTHuIREzszS4h21mlgZP6zMzS0U/J2xJy4GIiGFJLwFWAHdExIbSW2dm1q36DmG3rZp+DvBJ4FOS/gr4G2Bv4ExJZ09xnovwmtm0iLGJjpfUtOth/w5wKDAHeBBYFBGPSboQuAE4f7KTXITXzKZNenm4Y+0S9lhEjAOPS7o7Ih4DiIgnJNX4X4uZparONx3bVU1/StLcfP2VOzdKeja1/jlmZsma6GJpQ9IKSXdK2iLpzEn2nyLpYUkb8+W0hn2rJN2VL6uK+GrtethHRMSTABG7FEqbDRTSADOzIhXVw5Y0AFwCHAOMAsOShiJic9OhX4iIM5rOfQ5wDrCMbFj4xvzcR3pp05Q97J3JepLtP4qIW3sJbGZWiuJ62MuBLRGxNSKeAtYDx3fYitcDV0fE9jxJX002w64n7YZEzMySEmOdL40z2vJlsOFSC4H7Gz6P5tua/bakWyRdKWlxl+d2xQ/OmFmtRBd315pmtDXTZKc0ff5XYF1EPCnp7cDlwOs6PLdr7mGbWb0UNyQyCixu+LwI2NZ4QET8uGHo+NP8fHJG23N3hxO2mdVKTHS+tDEMLJW0RNKewEpgqPEASQc2fDwOuD1fvwo4VtJ8SfOBY/NtPfGQiJnVSjdDIlNeJ2JM0hlkiXYAWBsRmySdB4xExBDwLknHAWPAduCU/Nztkj5MlvQBzouInitCO2HbLnb8aGslcT4zciGnLXtfJbGsv8T4ZMPHu3mt7J1JG5q2fahh/SzgrBbnrgXWFtYYQBGlPxVU38eOaqaqZL3T7AWHVBrPktBztn3wiCM7zjkHfOu64rJ7BdzDNrNaiYmkcnBXnLDNrFaKGsOeiZywzaxWItzDNjNLgnvYZmaJmChwlshM44RtZrXim45mZomoc8Lu+tF0SZ8toyFmZkWI6HxJzZQ9bElDzZuA10raByAijiurYWZmu6POPex2QyKLgM3AZ8ieWBRZBYWPTXVS/k7ZQYDVq1czODg41eFmZoXp52l9y4B3A2cD74uIjZKeiIhvTnWSq6ab2XQZ79dZInkdx4skXZH/+cN255iZTad+7mEDEBGjwAmSfgN4rNwmmZntvn4ew95FRHwV+GpJbTEz61mKsz865eENM6sV97DNzBIxPlHfyodO2GZWKx4SMTNLxES/zxIxM0tF30/rMzNLhYdEEvDeg1dWFutZMVBZrHv008piQVbNvCpVFP19/7IPlh5jp0cZqyxWlVaPfLSyWEUUZvaQiPWNqiqZV12h3fqHZ4mYmSWixiMiTthmVi8eEjEzS4RniZiZJaLGRdOdsM2sXgL3sM3MkjDmIREzszS4h52T9KvAcuC2iPh6OU0yM9t9dR7DnnKGuaTvNqy/DfgbYB5wjqQzS26bmVnXAnW8pKbdI0GzG9YHgWMi4lzgWOD3Wp0kaVDSiKSRNWvWtDrMzKxwE10s7UhaIelOSVsm66RKeq+kzZJukXSNpOc17BuXtDFfhgr4am2HRGZJmk+W2BURDwNExP9JavniBFdNN7PpMl5Qz1nSAHAJcAwwCgxLGoqIzQ2H3QQsi4jHJf0h8FHgxHzfExFxaCGNybXrYT8buBEYAZ4j6QAASXtDgr9PmFntTajzpY3lwJaI2BoRTwHrgeMbD4iIf4+Ix/OP1wOLiv4+jabsYUfEwS12TQBvLrw1ZmY9muiiLylpkGy4d6c1+QgBwELg/oZ9o8CrprjcqcC/NXx+hqQRYAy4ICL+ueOGtbBb0/rynyj39BrczKxo3YzBNg3fNpss8096eUm/DywDfq1h80ERsU3SIcC1km6NiLu7aN7T1Pc9hGbWlwq86TgKLG74vAjY1nyQpKOBs4HjIuLJndsjYlv+51bgOuCw7r/NrpywzaxWJqSOlzaGgaWSlkjaE1gJ7DLbQ9JhwGqyZP1Qw/b5kubk6wuA1wCNNyt3i590NLNaGS/oOhExJukM4CpgAFgbEZsknQeMRMQQcCGwN3CFsh8A90XEccCLgdWSJsg6xhc0zS7ZLU7YZlYrHcz+6FhEbAA2NG37UMP60S3O+zbw8uJaknHCNrNa6WaWSGpqlbDPv/jwSuIMnz5cSRyADwxsryxWlaoqjvvRkb+sJA7AK1/W8uHfwr1lzvMri1VlIeOL7l3f8zXq/KRebRJ2VcnazGa2IodEZpraJGwzM6j32/qcsM2sVsbdwzYzS4N72GZmiXDCNjNLRI1LOjphm1m9uIdtZpaIoh5Nn4mcsM2sVuo8D7tdEd5XSXpWvv5MSedK+ldJH5H07GqaaGbWuSJrOs407V6vuhbYWf7mYrKSYR/Jt11WYrvMzHZLPyfsWRGxs9jusoh4T0T8Z145/ZBWJ7lquplNl+hiSU27MezbJL01Ii4Dbpa0LCJGJL0Q2NHqJFdNN7Pp0rdj2MBpwK9Juht4CfAdSVuBT+f7zMxmlPEultS0q5r+KHCKpHlkQyB7AKMR8cMqGmdm1q2JGv9S39G0voj4H+DmkttiZtazFG8mdsrzsM2sVurbv3bCNrOacQ/bzCwRY6pvH9sJ28xqpb7p2gnbzGrGQyIJmHfCxZXFesW+SyqLtenHP6gsVpUeZaz9QQWospL5jbd9vrJYbzjsDyuLddDAvMpiFaHvp/WZmaWivunaCdvMasZDImZmiRivcR/bCdvMasU9bDOzRIR72GZmaXAP28wsEXWe1tfufdhmZkkpsuKMpBWS7pS0RdKZk+yfI+kL+f4bJB3csO+sfPudkl7f8xfDPWwzq5mxgnrYkgaAS4BjgFFgWNJQRGxuOOxU4JGIeIGklWQ1b0+U9BJgJfBS4LnANyS9MCJ6qpvQrmr6uyQt7iWAmVmVoot/2lgObImIrRHxFLAeOL7pmOOBy/P1K4GjJCnfvj4inoyIe4At+fV60m5I5MPADZL+Q9IfSdqvk4u6CK+ZTZduqqY35qp8GWy41ELg/obPo/k2JjsmL1j+KLBvh+d2rd2QyFbglcDRwInAuZJuBNYBX8or0TyNi/Ca2XTpZlpfU65qNlk53+aLtzqmk3O71q6HHRExERFfj4hTycZi/hZYQZbMzcxmlG562G2MAo1DwouAba2OkbQH8Gxge4fndq1dwt7lp0RE7IiIoYg4CTio1+BmZkUbj+h4aWMYWCppiaQ9yW4iDjUdMwSsytd/B7g2IiLfvjKfRbIEWAp8t9fv1m5I5MRWOyLiiV6Dm5kVrah52BExJukM4CpgAFgbEZsknQeMRMQQcCnwD5K2kPWsV+bnbpL0RWAzMAa8o9cZItAmYUfE93sNYGZWpSIfTY+IDcCGpm0falj/KXBCi3PPB84vrDF4HraZ1YwfTTczS0SdH013wjazWvHb+szMEtHB7I9kOWGbWa14SCQBFx7w2spiza3wrsa1+y+oLlgNvWXO8yuLVWUl83+76VOVxTp92fsri1UE33Q0M0uEx7DNzBLhIREzs0SEbzqamaVh3D1sM7M0eEjEzCwRHhIxM0uEe9hmZono22l9DS/t3hYR35D0FuBXgNuBNRGxo4I2mpl1rJ8fTb8sP2aupFXA3sCXgKPIKgCvmuJcM7PK9fOQyMsj4hV5rbL/Bp4bEeOSPgfc3OqkvPLwIMDq1asZHBxsdaiZWaH6OWHPyodF9gLm8vMCk3OA2a1OctV0M5su/TxL5FLgDrJ6ZmcDV0jaCrwaWF9y28zMuta3PeyIuEjSF/L1bZI+CxwNfDoieq4AbGZWtL6dJQJZom5Y/wlwZaktMjPrwXjU9wWrnodtZrXSz2PYZmZJ6dsxbDOz1PT1GLaZWUomPCRiZpYG97DNzBLhWSIJuG/WWGWx9po1q7JYcxmoLFaVVo98tJI471/2wUriABw0MK+yWFVWMq/q76ooHhIxM0uEh0TMzBLhHraZWSLcwzYzS8R4jE93E0pT3d0zM7MKRETHSy8kPUfS1ZLuyv+cP8kxh0r6jqRNkm6RdGLDvr+XdI+kjflyaLuYTthmVisTRMdLj84EromIpcA1+edmjwMnR8RLgRXAJyTt07D/fRFxaL5sbBfQCdvMaqWqHjZwPHB5vn458KZJ2vL9iLgrX98GPATst7sBnbDNrFYmIjpeJA1KGmlYuqlnuH9EPACQ//kLUx0saTmwJ3B3w+bz86GSiyTNaRew7U1HSc8H3gwsBsaAu4B1EfFou3PNzKrWzSyRpnKGTyPpG8ABk+w6u5s2SToQ+AdgVcTPHsU8C3iQLImvAT4AnDfVdaZM2JLeBfwm8E3gl4CNZIn7O5L+KCKu66bRZmZlK/LR9Ig4utU+ST+UdGBEPJAn5IdaHPcs4KvAn0bE9Q3XfiBffVLSZcCftGtPuyGRtwErIuIvyEqDvSQiziYbPL9oii/ys18z1qxp+cPLzKxwFY5hDwGr8vVVwL80H5AXMf8y8NmIuKJp34H5nyIb/76tXcBO5mHvAYyTVUqfBxAR90ly1XQzm3EqfNLxAuCLkk4F7gNOAJC0DHh7RJwG/C5wBLCvpFPy807JZ4R8XtJ+gMhGL97eLmC7hP0ZYFjS9XnQj+QN2g/Y3t13MzMrX1UlwiLix8BRk2wfAU7L1z8HfK7F+a/rNma7qukX54PuLwY+HhF35NsfJkvgZmYzSl+XCIuITcCmCtpiZtYzF+E1M0uECxiYmSXCr1c1M0uEh0TMzBLh92GbmSXCPWwzs0TUeQy7q8c4q1yAwTrFcay0YtXxO9U5Vr8sM/n1qt285jCFOI6VVqw6fqc6x+oLMzlhm5lZAydsM7NEzOSEXdV7Wat8/6tjpROrjt+pzrH6gvKbA2ZmNsPN5B62mZk1cMI2M0vEjEvYklZIulPSFklnlhhnraSHJLUty1NArMWS/l3S7ZI2SXp3ibGeIem7km7OY51bVqw83oCkmyR9peQ490q6VdJGSSMlx9pH0pWS7sj/zn65pDgvyr/PzuUxSe8pKdYf5/893CZpnaRnlBEnj/XuPM6msr5P35ruieBNE+0HyErAH0JWSfhmsjqSZcQ6AjgcuK2C73UgcHi+Pg/4fonfS8De+fps4Abg1SV+t/cC/wh8peR/h/cCC8r+u8pjXQ6clq/vCexTQcwBsgrazyvh2guBe4Bn5p+/SFamqozv8TKy2oRzyZ6k/gawtIq/t35YZloPezmwJSK2RsRTwHrg+DICRcS3qKjMWUQ8EBHfy9f/B7id7H+iMmJFRPxv/nF2vpRyZ1nSIuA3yErJ1UJe4foI4FKAiHgqIn5SQeijgLsj4gclXX8P4JmS9iBLpttKivNi4PqIeDwixoBvAm8uKVbfmWkJeyFwf8PnUUpKbNNF0sHAYWQ937JiDEjaCDwEXB0RZcX6BPB+oIo3xgfwdUk3SirzCbpDgIeBy/Khns9I2qvEeDutBNaVceGI+G/gr8kKxT4APBoRXy8jFlnv+ghJ+0qaC/w6sLikWH1npiVsTbKtNvMOJe0N/BPwnoh4rKw4ETEeEYcCi4Dlkl5WdAxJbwQeiogbi752C6+JiMOBNwDvkFRWTdE9yIbKPhURhwH/B5R2LwVA0p7AccAVJV1/PtlvqkuA5wJ7Sfr9MmJFxO1kxbqvBr5GNqw5VkasfjTTEvYou/40XkR5v7pVStJssmT9+Yj4UhUx81/lrwNWlHD51wDHSbqXbOjqdZImrQ5dhIjYlv/5EPBlsuGzMowCow2/lVxJlsDL9AbgexHxw5KufzRwT0Q8HBE7gC8Bv1JSLCLi0og4PCKOIBt2vKusWP1mpiXsYWCppCV5r2MlMDTNbeqZJJGNid4eER8vOdZ+kvbJ159J9j/rHUXHiYizImJRRBxM9vd0bUSU0muTtJekeTvXgWPJfvUuXEQ8CNwv6UX5pqOAzWXEanASJQ2H5O4DXi1pbv7f4lFk91FKIekX8j8PAn6Lcr9bX5lR78OOiDFJZwBXkd01XxtZ1fbCSVoHHAkskDQKnBMRl5YRi6w3+gfArfnYMsAHI2JDCbEOBC6XNED2A/mLEVHqlLsK7A98Ocs17AH8Y0R8rcR47wQ+n3catgJvLStQPs57DHB6WTEi4gZJVwLfIxueuIlyHxv/J0n7AjuAd0TEIyXG6it+NN3MLBEzbUjEzMxacMI2M0uEE7aZWSKcsM3MEuGEbWaWCCdsM7NEOGGbmSXi/wHLmOzJ5zkBjQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##\n",
    "pd_corr = corr_matrix.collect()[0][\"pearson({})\".format(\"features\")].toArray().tolist()\n",
    "pd_corr = pd.DataFrame(pd_corr)\n",
    "sb.heatmap(pd_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes, I am going to choose a few continuous columns to keep which I believe could be important, please note this is not the best method, but some domain knowledge never hurt either! You can read more about each feature here: http://www.netflowmeter.ca/netflowmeter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Fwd Avg Bulk Rate', ' Bwd Avg Packets/Bulk', ' Active Std', ' Max Packet Length', ' ECE Flag Count', ' Flow IAT Max', ' Flow Packets/s', ' min_seg_size_forward', ' Subflow Bwd Bytes', ' Fwd IAT Min', 'Fwd Packets/s', 'FIN Flag Count', 'Total Length of Fwd Packets', ' Flow Duration', ' Fwd IAT Mean', ' URG Flag Count', 'Bwd IAT Total', ' Packet Length Std', ' Min Packet Length', ' Idle Max', ' Bwd Avg Bytes/Bulk', ' Avg Bwd Segment Size', ' act_data_pkt_fwd', ' Fwd Packet Length Min', 'Flow Bytes/s', ' Total Fwd Packets', ' Packet Length Variance', ' ACK Flag Count', ' Fwd IAT Max', ' Total Length of Bwd Packets', ' Bwd Header Length', ' Fwd Packet Length Max', ' Bwd IAT Std', 'Bwd Packet Length Max', ' Fwd Packet Length Std', ' Bwd IAT Max', 'Bwd Avg Bulk Rate', ' Flow IAT Std', ' Subflow Bwd Packets', ' RST Flag Count', ' Flow IAT Mean', ' Fwd Avg Packets/Bulk', ' Bwd Packet Length Mean', ' Active Max', ' Fwd IAT Std', 'Subflow Fwd Packets', ' SYN Flag Count', ' Idle Std', ' Subflow Fwd Bytes', ' Active Min', 'Idle Mean', ' CWE Flag Count', ' Avg Fwd Segment Size', ' Total Backward Packets', 'Fwd Avg Bytes/Bulk', ' PSH Flag Count', ' Flow IAT Min', ' Bwd Packet Length Std', ' Init_Win_bytes_backward', ' Bwd Packets/s', 'Init_Win_bytes_forward', ' Down/Up Ratio', ' Bwd Packet Length Min', ' Fwd Packet Length Mean', ' Fwd Header Length55', 'Fwd IAT Total', ' Average Packet Size', ' Idle Min', ' Bwd IAT Mean', 'Active Mean', ' Packet Length Mean', ' Fwd Header Length34', ' Bwd IAT Min']\n"
     ]
    }
   ],
   "source": [
    "print(cont_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_cols = ['Fwd Packets/s', ' Bwd Packets/s', ' Max Packet Length', ' Total Fwd Packets', ' ACK Flag Count',\\\n",
    "               ' RST Flag Count', 'Fwd Avg Bytes/Bulk', ' Down/Up Ratio', ' Active Min', ' Flow IAT Std']\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=chosen_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid='skip')\n",
    "vectorizedDF = assembler.transform(df)\n",
    "\n",
    "standardScaler = StandardScaler().setInputCol(\"features\").setOutputCol(\"scaled_features\")\n",
    "scaledDF = standardScaler.fit(vectorizedDF).transform(vectorizedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "##After this cell, the outputs are scaled to a reasonable size to aid our algorithm in the next steps\n",
    "##There are many different methods of scaling including manually setting limits (MinMaxScaler), dealing with outliers\n",
    "##(RobustScaler), or simple Normalizer.\n",
    "standardScaler = StandardScaler().setInputCol(\"features\").setOutputCol(\"scaled_features\")\n",
    "scaledDF = standardScaler.fit(vectorizedDF).transform(vectorizedDF)\n",
    "#scaledDF.select(\"scaled_features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we are done with continuous variables and have already created a dealt with our tricky categorical variable Ports. If I had more time, I would spend it considering their correlations and trying to eliminate some of the columns, but for now I am just going to use all of them! Lets combine our categorical and continuous variables into one feature vector and move on to the model itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features| Label|\n",
      "+--------------------+------+\n",
      "|(10,[0,1,2,3,9],[...|BENIGN|\n",
      "|(10,[0,1,2,3,9],[...|BENIGN|\n",
      "|(10,[0,1,2,3,9],[...|BENIGN|\n",
      "|(10,[0,1,2,3,9],[...|BENIGN|\n",
      "|(10,[0,1,2,3,9],[...|BENIGN|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#combine our continuous and categorical columns into one list\n",
    "all_cols = categ_cols.append(\"scaled_features\")\n",
    "\n",
    "#new asssembler to combine these columns into a single vector\n",
    "combiner = VectorAssembler(\n",
    "    inputCols=all_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid='skip')\n",
    "\n",
    "finalDF = assembler.transform(df).select('features', \" Label\")\n",
    "finalDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline \n",
    "In reality, although it is helpful to walk through all those steps conceptually, they can all be assembled into a pipeline and shipped as one job. We will use PySpark's ml library to combine all these steps, and later add our model to the pipeline to show an end-to-end transition.  \n",
    "  \n",
    "While you can write a custom transformer to perform unsupported logic (such as our count-based bucketizing of port data) (more information here: https://danvatterott.com/blog/2019/07/12/limiting-cardinality-with-a-pyspark-custom-transformer/), for now we will begin the pipeline after that change has been made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "\n",
    "cont_cols = ['Fwd Packets/s', ' Bwd Packets/s', ' Max Packet Length', ' Total Fwd Packets',\\\n",
    "             ' ACK Flag Count',' RST Flag Count', 'Fwd Avg Bytes/Bulk', ' Down/Up Ratio',\\\n",
    "             ' Active Min', ' Flow IAT Std']\n",
    "\n",
    "#combines continuous features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=cont_cols,\n",
    "    outputCol=\"continuous_vars\",\n",
    "    handleInvalid='skip')\n",
    "\n",
    "#scales continuous features\n",
    "standardScaler = StandardScaler().setInputCol(\"continuous_vars\")\\\n",
    ".setOutputCol(\"scaled_features\")\n",
    "\n",
    "#Combines categorical with our continuous features\n",
    "combiner = VectorAssembler(\n",
    "    inputCols=['bucketedPorts', 'Fwd PSH Flags', ' Bwd PSH Flags', ' Fwd URG Flags',\\\n",
    "               ' Bwd URG Flags', \"scaled_features\"],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid='skip')\n",
    "\n",
    "##Finally we map the data labels to integers\n",
    "label_index = StringIndexer(inputCol = \" Label\", outputCol = 'label', handleInvalid = 'skip').fit(df)\n",
    "\n",
    "#combine all stages into an array\n",
    "stages+=[assembler, standardScaler, combiner, label_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.drop()\n",
    "\n",
    "#Create and fit the pipeline\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "\n",
    "##final_df is the result of applying each transformation in order to df\n",
    "final_df = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label', 'features', 'bucketedPorts', 'Fwd PSH Flags', ' Bwd PSH Flags', ' Fwd URG Flags', ' Bwd URG Flags', 'scaled_features', 'Fwd Packets/s', ' Bwd Packets/s', ' Max Packet Length', ' Total Fwd Packets', ' ACK Flag Count', ' RST Flag Count', 'Fwd Avg Bytes/Bulk', ' Down/Up Ratio', ' Active Min', ' Flow IAT Std']\n",
      "root\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- bucketedPorts: float (nullable = true)\n",
      " |-- Fwd PSH Flags: float (nullable = true)\n",
      " |--  Bwd PSH Flags: float (nullable = true)\n",
      " |--  Fwd URG Flags: float (nullable = true)\n",
      " |--  Bwd URG Flags: float (nullable = true)\n",
      " |-- scaled_features: vector (nullable = true)\n",
      " |-- Fwd Packets/s: float (nullable = true)\n",
      " |--  Bwd Packets/s: float (nullable = true)\n",
      " |--  Max Packet Length: float (nullable = true)\n",
      " |--  Total Fwd Packets: float (nullable = true)\n",
      " |--  ACK Flag Count: float (nullable = true)\n",
      " |--  RST Flag Count: float (nullable = true)\n",
      " |-- Fwd Avg Bytes/Bulk: float (nullable = true)\n",
      " |--  Down/Up Ratio: float (nullable = true)\n",
      " |--  Active Min: float (nullable = true)\n",
      " |--  Flow IAT Std: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categ_cols = ['bucketedPorts', 'Fwd PSH Flags', ' Bwd PSH Flags', ' Fwd URG Flags',\\\n",
    "              ' Bwd URG Flags']\n",
    "\n",
    "all_cols = categ_cols + cont_cols\n",
    "#categ_cols.append(cont_cols)\n",
    "\n",
    "\n",
    "cols = ['label', 'features']\n",
    "cols = cols + all_cols\n",
    "print(cols)\n",
    "final_df = final_df.select(cols)\n",
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to split our data into train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 334383\n",
      "Test Dataset Count: 111325\n"
     ]
    }
   ],
   "source": [
    "train, test = final_df.randomSplit([0.75, 0.25], seed = 42)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection & Results\n",
    "Pyspark has a few basic models built in which are configured to work with Spark DataFrames and can be integrated with our previous pipeline. They are currently somewhat limited, but for our example we will begin using one of them, a simple Random Forest MultiClass Classifier. (https://spark.apache.org/docs/latest/ml-classification-regression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "##We can think of these as two additions to our PipeLine, and can add them to our PipeLine\n",
    "\n",
    "#numTrees is a hyperparameter\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol='features', numTrees=10)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=label_index.labels)\n",
    "\n",
    "truelabelConverter = IndexToString(inputCol=\"label\", outputCol=\"trueLabel\",\n",
    "                               labels=label_index.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trueLabelConverter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-f56435e85955>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabelConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrueLabelConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trueLabelConverter' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "OR before we run our pipeline above, these stages could simply be added to the end and run\n",
    "together, however to be able to examine summaries of the model conveniently, I am going to \n",
    "split them up. Code to use in pipeline is below:\n",
    "\n",
    "stages = [rf, labelConverter]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(test)\n",
    "'''\n",
    "model = rf.fit(train)\n",
    "predictions = model.transform(test)\n",
    "\n",
    "predictions = labelConverter.transform(predictions)\n",
    "predictions = truelabelConverter.transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+--------------------+\n",
      "|predictedLabel|trueLabel|            features|\n",
      "+--------------+---------+--------------------+\n",
      "|        BENIGN|   BENIGN|(15,[0,1,5,6,7,8,...|\n",
      "|        BENIGN|   BENIGN|(15,[0,1,5,6,7,8,...|\n",
      "|        BENIGN|   BENIGN|(15,[0,1,5,6,7,8,...|\n",
      "|        BENIGN|   BENIGN|(15,[0,1,5,6,7,8,...|\n",
      "|        BENIGN|   BENIGN|(15,[0,1,5,6,7,8,...|\n",
      "+--------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"trueLabel\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.00673703\n"
     ]
    }
   ],
   "source": [
    "#Code to evaluate our model by comparing our predictions to true labels\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already at less than 1% error on our test set! Not bad at all, let's breakdown which labels we do the worst in and then move on to further optimizing our methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('label', 'double'),\n",
       " ('features', 'vector'),\n",
       " ('bucketedPorts', 'float'),\n",
       " ('Fwd PSH Flags', 'float'),\n",
       " (' Bwd PSH Flags', 'float'),\n",
       " (' Fwd URG Flags', 'float'),\n",
       " (' Bwd URG Flags', 'float'),\n",
       " ('scaled_features', 'vector'),\n",
       " ('Fwd Packets/s', 'float'),\n",
       " (' Bwd Packets/s', 'float'),\n",
       " (' Max Packet Length', 'float'),\n",
       " (' Total Fwd Packets', 'float'),\n",
       " (' ACK Flag Count', 'float'),\n",
       " (' RST Flag Count', 'float'),\n",
       " ('Fwd Avg Bytes/Bulk', 'float'),\n",
       " (' Down/Up Ratio', 'float'),\n",
       " (' Active Min', 'float'),\n",
       " (' Flow IAT Std', 'float'),\n",
       " ('rawPrediction', 'vector'),\n",
       " ('probability', 'vector'),\n",
       " ('prediction', 'double'),\n",
       " ('predictedLabel', 'string'),\n",
       " ('trueLabel', 'string')]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error for class BENIGN = 0.0\n",
      "Test Error for class FTP-Patator = 0.005402750491159125\n",
      "Test Error for class SSH-Patator = 0.4913563829787234\n"
     ]
    }
   ],
   "source": [
    "for label in ['BENIGN', 'FTP-Patator', 'SSH-Patator']:\n",
    "    temp = predictions[predictions.trueLabel == label]\n",
    "    accuracy = evaluator.evaluate(temp)\n",
    "    print(\"Test Error for class \"+label+\" = \"+str(1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, our accuracy within each class is much smaller for the lesser represented classes. There are a few ways to improve on this simple model including:  \n",
    ">-Better Feature Selection and more created features  \n",
    ">-Include more features  \n",
    ">-Use more of the provided dataset (this analyses only uses Tuesday)\n",
    ">-Data Augmentation/Up-sampling  \n",
    ">-Try different models  \n",
    ">-Tune hyperparameters (NEXT)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparam Optimization  \n",
    "I found this feature within PySpark's ml library and it has been a godsend ever since. Automated hyperparameter tuning is enabled by using the CrossValidator object and building a parameter grid to search over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: auto)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)\n",
      "labelCol: label column name. (default: label, current: label)\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)\n",
      "numTrees: Number of trees to train (>= 1). (default: 20, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "seed: random seed. (default: -5387697053847413545)\n",
      "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\n"
     ]
    }
   ],
   "source": [
    "#here we can view the parameters, and select ranges to try for each\n",
    "print(rf.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaModelWrapper.__del__ at 0x1a3a5a8e18>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/bgriffin/spark-2.4.3-bin-hadoop2.7/python/pyspark/mllib/common.py\", line 142, in __del__\n",
      "    self._sc._gateway.detach(self._java_model)\n",
      "AttributeError: 'MultilabelMetrics' object has no attribute '_sc'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9996586570851111"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Here, we choose a few of the parameters of the RandomForest model we wish to optimize, and\n",
    "build a ParamGrid over which the CrossValidator can search for the best combination.\n",
    "\n",
    "The best model is returned as cv, and can be used to run classification on your dataset.\n",
    "\n",
    "NOTE: This may take a while to run, you can reduce the search space or number of folds to\n",
    "reduce runtime, but with less options the optimization may not be as good\n",
    "'''\n",
    "\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [4, 6, 8])\n",
    "             .addGrid(rf.numTrees, [10, 15, 20])\n",
    "             .addGrid(rf.subsamplingRate, [1.0, 0.9, 0.75])\n",
    "             .build())\n",
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "# Run cross validations.  This can take about 6 minutes since it is training over 20 trees!\n",
    "cvModel = cv.fit(train)\n",
    "predictions = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9996586570851111"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = labelConverter.transform(predictions)\n",
    "predictions = truelabelConverter.transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error for class BENIGN = 0.00011133274574381957\n",
      "Test Error for class FTP-Patator = 0.0019646365422396617\n",
      "Test Error for class SSH-Patator = 0.0146276595744681\n"
     ]
    }
   ],
   "source": [
    "for label in ['BENIGN', 'FTP-Patator', 'SSH-Patator']:\n",
    "    temp = predictions[predictions.trueLabel == label]\n",
    "    accuracy = evaluator.evaluate(temp)\n",
    "    print(\"Test Error for class \"+label+\" = \"+str(1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the parameter optimization, two of the three labels (notably, the lesser represented classes) achieved lower test error, with a 99.96% overall accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
